
#+TITLE: Neural Networks & Facial Recognition
#+AUTHOR: Asher Mancinelli
#+EMAIL: ashermancinelli@gmail.com

#+DESCRIPTION: Talk on the mathematics behind neural networks and 
#+LANGUAGE: en
#+OPTIONS:   H:2 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:https://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 

#+STARTUP: indent
#+STARTUP: hidestars

#+STARTUP: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]
#+LaTeX_HEADER: \usepackage{amsmath}

# This means that beamer will export second level headlines as frames, 
# and first level headlines as sections in the presentation.
#+BEAMER_FRAME_LEVEL: 2

# The [[https://orgmode.org/worg/exporters/beamer/tutorial.html][org-mode presentation tutorial]] is where many of these mysterious commands come from, reference
# this document for this line. Has something to do with formatting the latex into beamer.
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)


# Intended Outline:
# - Motivation & Use Cases
#   - History
#   - Why Neural Networks?
# - Theory 
#   - Mathematics
#   - Computer Science
#   - Image Processing
# - Implementation
#   - Lets Write A NN!
#   - In Production
# - Conclusion
#   - Questions?

* Introduction

** Who Am I?

#+BEGIN_LATEX
\begin{center}
  \textsc{Asher Mancinelli} \\
  \textsc{Whitworth University, 2020} \\
  \textsc{BS Math, BS CS} \\
  \textsc{Research Computing Intern @ PNNL}
\end{center}
#+END_LaTeX

** Outline
- Motivation
- Theory
- Implementation (live coding? Yikes!)
- Conclusion
  
* Motivation

** Why Neural Networks?

- _Intractable Problem_: A problem that by its nature, is extremely difficut to cast in terms of the conventional, Von-Neumann computer architecture.$^{2}$
- AKA problems we humans don't think about, but are quite complex
  - Computer vision, facial recognition
  - Audio signals
  - Fuzzy logic

** History

# Note: This field is actually quite old, and dates back to Von Neumann. Began with the perceptron, which took 
# /n/ inputs with weights and an activation function. 
- _Perceptron_: Models a neuron \rightarrow takes inputs and passes through activation and threshold functions. 
- Not actually a new idea, dates back to John von Neumann in the 1950's$^1$
- Note: more on all this later...

** Some Terms

- _Multilayer Perceptron_: layers and layers of perceptrons all connected together, aka a /dense neural network/
- _Deep Learning_: Multilayer perceptron with \geq 3 layers is /deep/, therefor we get /deep learning/!
- _Neocognitron_: Early image processing neural network, predecessor to the /convolutional/ neural network$^{2}$

** History

Current state of deep learning
[[./images/big-data-sex.jpg]]

* Theory

** Math - My Favorite!

Remember the /perceptron/?

** Math - My Favorite!

[[./images/perceptron.png]] 
$$ output = f(\sum_{i=0}^{n} x_i \times w_i) $$

** Math - My Favorite!

- _Activation Function_: Scaling function for output of \sum of inputs
- Ex. _ReLU Activation Function_:
#+BEGIN_EXPORT latex
\[ \begin{cases} 
      0 & x \leq 0 \\
      x & x > 0
   \end{cases}
\]
#+END_EXPORT

** Math - My Favorite!

Lets hook a few of these together...
[[./images/mlp.jpg]]

* Implementation

** Implementation
Live coding! Whatever could go wrong?

** Too Many Choices!
[[./images/frameworks.png]]

** Keep It Simple
For today we'll use 
#+ATTR_LaTeX: width=0.8\textwidth
[[./images/keras-python.png]]

** Getting Started

#+name: simple-model
#+BEGIN_SRC python :exports code
import keras
from keras.layers import Sequential, Dense

model = Sequential()
model = Dense(4, activation='relu')
model = Dense(3, activation='relu')
model = Dense(1, activation='softmax')
model.compile(
    optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics='accuracy'
)
#+END_SRC

* Conclusion

** Questions?
**References**:
- 1: Luger, George F. Artificial Intelligence. 5th ed., Addison-Wesley, 2005.
- 2: Skapura, David M. Building Neural Networks. Addison-Wesley, 1996.
- 3: 
  
